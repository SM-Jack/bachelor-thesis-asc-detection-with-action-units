{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09cbc912",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xgboost\\lib\\site-packages\\pandas\\__init__.py:138\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    122\u001b[0m     concat,\n\u001b[0;32m    123\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     qcut,\n\u001b[0;32m    136\u001b[0m )\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xgboost\\lib\\site-packages\\pandas\\api\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     extensions,\n\u001b[0;32m      4\u001b[0m     indexers,\n\u001b[0;32m      5\u001b[0m     interchange,\n\u001b[0;32m      6\u001b[0m     types,\n\u001b[0;32m      7\u001b[0m     typing,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xgboost\\lib\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Expanding,\n\u001b[0;32m     21\u001b[0m     ExpandingGroupby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     Window,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[0;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameGroupBy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndexResamplerGroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xgboost\\lib\\site-packages\\pandas\\io\\json\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     read_json,\n\u001b[0;32m      3\u001b[0m     to_json,\n\u001b[0;32m      4\u001b[0m     ujson_dumps,\n\u001b[0;32m      5\u001b[0m     ujson_loads,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_dumps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_loads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_table_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xgboost\\lib\\site-packages\\pandas\\io\\json\\_json.py:71\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     build_table_schema,\n\u001b[0;32m     69\u001b[0m     parse_table_schema,\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m         Hashable,\n\u001b[0;32m     76\u001b[0m         Mapping,\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xgboost\\lib\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     TextFileReader,\n\u001b[0;32m      3\u001b[0m     TextParser,\n\u001b[0;32m      4\u001b[0m     read_csv,\n\u001b[0;32m      5\u001b[0m     read_fwf,\n\u001b[0;32m      6\u001b[0m     read_table,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextParser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xgboost\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     AbstractMethodError,\n\u001b[0;32m     35\u001b[0m     ParserWarning,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
      "File \u001b[1;32mparsers.pyx:1418\u001b[0m, in \u001b[0;36minit pandas._libs.parsers\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b54ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c856978",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ff4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = {}\n",
    "\n",
    "for (root, dirs, file) in os.walk(\"../all_features\"):\n",
    "    for f in file:\n",
    "        if \".csv\" in f:\n",
    "            path = root + \"/\" + f\n",
    "            df = pd.read_csv(path, index_col=[0,1,2])\n",
    "            # Remove \"Unnamed\" columns\n",
    "            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "            data[f] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data[\"91-001_part_2_concat.csv\"]\n",
    "example.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.hist([\"libreface_AU04_i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mean = example.filter(like=\"openface\").mean()\n",
    "#print(example_mean[:5])\n",
    "#print(type(example_mean))\n",
    "\n",
    "# Rename columns\n",
    "example_mean = example_mean.rename(lambda x: x + '_mean')\n",
    "print(example_mean[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015de6a",
   "metadata": {},
   "source": [
    "## Mean, var, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aced74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def participant_stats(df, method_filter, percentage_filter): #percentage meaning intensity filter\n",
    "        \n",
    "    # mean\n",
    "    columns = df.loc[:, df.columns.str.match(method_filter)]\n",
    "    means = columns.mean()\n",
    "    # Rename columns\n",
    "    means = means.rename(lambda x: x.replace(method_filter, \"\") + '_mean')\n",
    "\n",
    "    # variance\n",
    "    columns = df.loc[:, df.columns.str.match(percentage_filter)]\n",
    "    m_vars = columns.var()\n",
    "    m_vars = m_vars.rename(lambda x: x.replace(method_filter, \"\") + '_var')\n",
    "\n",
    "    # median\n",
    "    med = columns.median()\n",
    "    med = med.rename(lambda x: x.replace(method_filter, \"\") + '_median')\n",
    "\n",
    "    stat = pd.concat([means, m_vars, med])\n",
    "    \n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(data, method_filter='openface_', percentage_filter=r'openface_.*_r'):\n",
    "    \"\"\"\n",
    "    data = whole dataset\n",
    "    method_filter = which method to filter out\n",
    "    percentage_filter = all relevant AUs - must be filtered by regular expression\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    print(\"iterate and filter through every AU beginning with \", percentage_filter)\n",
    "\n",
    "    for k in list(data.keys()):\n",
    "        df = data[k]\n",
    "        stat = participant_stats(df, method_filter, percentage_filter)\n",
    "        stats[k] = stat\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398693e1",
   "metadata": {},
   "source": [
    "### get stats of each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a93e4-4407-4198-83bc-30d642ad95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame with only columns containing \"openface_\"\n",
    "data_of = {}\n",
    "for key, df in data.items():\n",
    "    data_of[key] = df.loc[:, df.columns.str.contains(r'openface_|timestamp')].copy()\n",
    "\n",
    "#data_of[\"91-001_part_2_concat.csv\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a624ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open_face_stats = get_stats(data_of, 'openface_', r'openface_.*_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864fbd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open_face_stats[\"91-001_part_2_concat.csv\"]#.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a21d4-806e-49b8-9074-87fe9f633bf9",
   "metadata": {},
   "source": [
    "##### Some Libreface-values must be converted to binary values first\n",
    "\n",
    "_d is BP4D <-> indicating AU presence\n",
    "\n",
    "_i is DISFA <-> intensity estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db3535-6c78-411d-b39d-54119a5634d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame with only columns containing \"libreface_\"\n",
    "data_lf = {}\n",
    "for key, df in data.items():\n",
    "    data_lf[key] = df.loc[:, df.columns.str.contains(r'libreface_|timestamp')].copy()\n",
    "\n",
    "# Create new columns with binary values\n",
    "for key, df in data_lf.items():\n",
    "    # Columns matching the pattern\n",
    "    lf_presence_columns = df.columns[df.columns.str.match(r'libreface_.*_d')]\n",
    "    # Apply binary conversion with threshold 0.5\n",
    "    data_lf[key].loc[:, lf_presence_columns] = df[lf_presence_columns].apply(lambda col: col.map(lambda x: 1 if x > 0.5 else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "libre_face_stats = get_stats(data_lf, 'libreface_', r'libreface_.*_i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb2bf5-0cc0-4d34-805d-d42798fedc18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "libre_face_stats[\"91-001_part_2_concat.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baab4b-8fde-4ecf-9a81-666f5a94723b",
   "metadata": {},
   "source": [
    "##### Create columns with binary values for ME Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308f938-80ff-4213-90f9-2cb17776d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns containing \"presence\" from the original DataFrame\n",
    "#df = df.loc[:, ~df.columns.str.contains('presence')]\n",
    "\n",
    "# Create a copy of the DataFrame with only columns containing \"me_graph_\"\n",
    "data_me_graph = {}\n",
    "for key, df in data.items():\n",
    "    data_me_graph[key] = df.loc[:, df.columns.str.contains(r'me_graph_|timestamp')].copy()\n",
    "    \n",
    "\n",
    "# Create new columns with binary values\n",
    "for key, df in data_me_graph.items():\n",
    "    for col in df:\n",
    "        new_col_name = f\"{col}_presence\"\n",
    "        data_me_graph[key].loc[:, new_col_name] = data_me_graph[key][col].map(lambda x: 1 if x > 0.5 else 0)\n",
    "        \n",
    "print(type(data_me_graph))\n",
    "#print(data_me_graph.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db3c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (?!_presence\\b) means, that it is not allowed in the regex\n",
    "me_graph_stats = get_stats(data_me_graph, 'me_graph_', r'^me_graph_(?!.*presence)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53132c9d-fb7e-4523-ab96-d0285cac9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "me_graph_stats[\"91-001_part_2_concat.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b94fc",
   "metadata": {},
   "source": [
    "#### save in new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1670a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Model_Input/whole_video/\"\n",
    "\n",
    "of_frame = pd.DataFrame.from_dict(open_face_stats, orient='index')\n",
    "of_frame.reset_index(inplace=True)\n",
    "of_frame.rename(columns={'index': 'id'}, inplace=True)\n",
    "of_frame = of_frame.sort_values('id')\n",
    "of_frame.to_csv(path + \"openface_stats_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606de302",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_frame = pd.DataFrame.from_dict(libre_face_stats, orient='index')\n",
    "lf_frame.reset_index(inplace=True)\n",
    "lf_frame.rename(columns={'index': 'id'}, inplace=True)\n",
    "lf_frame = lf_frame.sort_values('id')\n",
    "lf_frame.to_csv(path + \"libreface_stats_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee79495",
   "metadata": {},
   "outputs": [],
   "source": [
    "me_frame = pd.DataFrame.from_dict(me_graph_stats, orient='index')\n",
    "me_frame.reset_index(inplace=True)\n",
    "me_frame.rename(columns={'index': 'id'}, inplace=True)\n",
    "me_frame = me_frame.sort_values('id')\n",
    "me_frame.to_csv(path + \"megraph_stats_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ab346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "#print(lf_frame['libreface_AU01_d_mean']) \n",
    "#print(of_frame['openface_AU01_r_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48545f89",
   "metadata": {},
   "source": [
    "# Partial sub-grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990d7652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stamp_overview \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../open_face_features_timestamps.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m new_part_rows \u001b[38;5;241m=\u001b[39m stamp_overview[stamp_overview[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m stamp_overview[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_part_rows)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "stamp_overview = pd.read_csv(\"../open_face_features_timestamps.csv\", sep = ';')\n",
    "new_part_rows = stamp_overview[stamp_overview['speaker'] != stamp_overview['speaker'].shift(1)]\n",
    "print(new_part_rows)\n",
    "\n",
    "frame_stamps = {\n",
    "    \"neutral_actress\": [0, 1000],\n",
    "    \"neutral_participant\": [1001, 1650],\n",
    "    \"joy_actress\": [1651, 2425],\n",
    "    \"joy_participant\": [2426, 3075],\n",
    "    \"disgust_actress\": [3075, 3900],\n",
    "    \"disgust_participant\": [3901, 4803]\n",
    "}\n",
    "\n",
    "time_stamps = {\n",
    "    \"neu_actress\": 40.0,\n",
    "    \"neutral_participant\": 66.00,\n",
    "    \"joy_actress\": 97.0,\n",
    "    \"joy_participant\": 123.0,\n",
    "    \"disgust_actress\": 156.0,\n",
    "    \"disgust_participant\": 192.08\n",
    "}\n",
    "# face=['AU04_r', 'AU06_r', 'AU09_r', 'AU12_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1ab33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create sub-groups for each part of the video\n",
    "def create_sub_groups(df):\n",
    "    sub_groups = {}\n",
    "    previous_time = 0.0\n",
    "    for label, time in time_stamps.items():\n",
    "        sub_group = df[(df['timestamp'] >= previous_time) & (df['timestamp'] <= time)]\n",
    "        sub_groups[label] = sub_group\n",
    "        previous_time = time\n",
    "    return sub_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f96ee-c34c-46fa-bda8-481b9e1e416d",
   "metadata": {},
   "source": [
    "##### Part each dataframe by timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parted_of = {}\n",
    "for key, df in data_of.items():\n",
    "    parted_of[key] = create_sub_groups(df)\n",
    "#parted_of = {'participant.csv': {'neu_actress': df ...}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c30b87-bd90-4f2a-b739-fb230e8c9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "parted_lf = {}\n",
    "for key, df in data_lf.items():\n",
    "    parted_lf[key] = create_sub_groups(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3bb08-9395-4ce6-9bcb-3ff5e920f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parted_me = {}\n",
    "# Create subgroups for every data\n",
    "for key, df in data_me_graph.items():\n",
    "    parted_me[key] = create_sub_groups(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46b7d5-fe58-4f78-a442-f02bfb50e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parted_me[\"91-001_part_2_concat.csv\"][\"joy_actress\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3ae20",
   "metadata": {},
   "source": [
    "### get stats of each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_stats(data, method_filter, au_filter):\n",
    "    stats = {}\n",
    "    for tstmp, df in data.items():\n",
    "        stat = participant_stats(df, method_filter, au_filter)\n",
    "        stats[tstmp] = stat\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae82cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = parted_of[\"91-001_part_2_concat.csv\"]\n",
    "example\n",
    "#get_part_stats(example, 'openface_', 'openface_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats for every participant\n",
    "parted_stats_openface = {}\n",
    "for key, time_data_dict in parted_of.items():\n",
    "    # key = participant; time_data_dict = 'neu_actress': df\n",
    "    parted_stats_openface[key] = get_part_stats(time_data_dict, 'openface_', r'openface_.*_r')\n",
    "    \n",
    "parted_stats_libreface = {}\n",
    "for key, time_data_dict in parted_lf.items():\n",
    "    parted_stats_libreface[key] = get_part_stats(time_data_dict, 'libreface_', r'libreface_.*_i')\n",
    "    \n",
    "parted_stats_megraph = {}\n",
    "for key, time_data_dict in parted_me.items():\n",
    "    parted_stats_megraph[key] = get_part_stats(time_data_dict, 'me_graph_', r'^me_graph_(?!.*presence)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382e2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = parted_stats_openface[\"91-001_part_2_concat.csv\"]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493804b9",
   "metadata": {},
   "source": [
    "## Restructure df\n",
    "Should have all participants in one file,\n",
    "'AU_part' as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fe621",
   "metadata": {},
   "source": [
    "### Invert\n",
    "Invert so that participants are inside, then create dataframe with participants as rows and AU+video_part together as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04706831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'A': {1: {'b': 'value'}}} -> {'b': {1: {'A': 'value'}}}\n",
    "def invert_dict_p_inside(d):\n",
    "    inverted = {}\n",
    "    for key, subdict in d.items():\n",
    "        for subkey, subsubdict in subdict.items():\n",
    "            for subsubkey, value in subsubdict.items():\n",
    "                if subkey not in inverted:\n",
    "                    inverted[subkey] = {}\n",
    "                if subsubkey not in inverted[subkey]:\n",
    "                    inverted[subkey][subsubkey] = {}\n",
    "                inverted[subkey][subsubkey][key] = value\n",
    "    return inverted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_dict_of = invert_dict_p_inside(parted_stats_openface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7168171",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_frame = pd.DataFrame.from_dict({(j, i): inverted_dict_of[i][j] \n",
    "                             for i in inverted_dict_of.keys() \n",
    "                             for j in inverted_dict_of[i].keys()},\n",
    "                            orient='columns')\n",
    "    \n",
    "of_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_dict_lf = invert_dict_p_inside(parted_stats_libreface)\n",
    "\n",
    "lf_frame = pd.DataFrame.from_dict({(j, i): inverted_dict_lf[i][j] \n",
    "                             for i in inverted_dict_lf.keys() \n",
    "                             for j in inverted_dict_lf[i].keys()},\n",
    "                            orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892472fc-918a-4e40-b8eb-8324f7462f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_dict_me = invert_dict_p_inside(parted_stats_megraph)\n",
    "\n",
    "me_frame = pd.DataFrame.from_dict({(j, i): inverted_dict_me[i][j] \n",
    "                             for i in inverted_dict_me.keys() \n",
    "                             for j in inverted_dict_me[i].keys()},\n",
    "                            orient='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413db42f",
   "metadata": {},
   "source": [
    "## save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Model_Input/parted_video/\"\n",
    "\n",
    "of_frame.to_csv(path + \"openframe_stats_parted.csv\")\n",
    "lf_frame.to_csv(path + \"libreface_stats_parted.csv\")\n",
    "me_frame.to_csv(path + \"me_graph_stats_parted.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
